Notes for GPTScore

............
assessing the
quality of the generation is an even more ardu-
ous task than the generation itself, and this is-
sue has not been given adequate consideration
recently. This paper proposes a novel evalua-
tion framework, GPTS CORE, which utilizes the
emergent abilities (e.g., zero-shot instruction) of
generative pre-trained models to score generated
texts. There are 19 pre-trained models explored
in this paper, ranging in size from 80M (e.g.,
FLAN-T5-small) to 175B (e.g., GPT3). Exper-
imental results on four text generation tasks, 22
evaluation aspects, and corresponding 37 datasets
demonstrate that this approach can effectively
allow us to achieve what one desires to evalu-
ate for texts simply by natural language instruc-
tions. This nature helps us overcome several
long-standing challenges in text evaluation–how
to achieve customized, multi-faceted evaluation
without the need for annotated samples. We make
our code publicly available. 1

............
1. Introduction
The advent of generative pre-trained models, such as GPT3
(Brown et al., 2020), has precipitated a shift from analyti-
cal AI to generative AI across multiple domains (Sequoia,
2022). Take text as an example: the use of a large pre-
trained model with appropriate prompts (Liu et al., 2021)
has achieved superior performance in tasks defined both in
academia (Sanh et al., 2021) and scenarios from the real
world (Ouyang et al., 2022).

............
Sequoia, T. Generative ai: A creative new world.
https://www.sequoiacap.com/article/
generative-ai-a-creative-new-world/,
2022.

............
arXiv:2107.13586,

............
Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing
Published on Jul 28, 2021
Authors:

Pengfei Liu
,
Weizhe Yuan
,

Jinlan Fu
,
Zhengbao Jiang
,
Hiroaki Hayashi
,
Graham Neubig
Abstract

This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.

............
arXiv:2203.02155

............
Training language models to follow instructions with human feedback

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe
Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.
Subjects:	Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
Cite as:	arXiv:2203.02155 [cs.CL]
 	(or arXiv:2203.02155v1 

............
While text generation tech-
nology is advancing rapidly, techniques for evaluating the

............
quality of these texts lag far behind. This is especially evi-
dent in the following ways:

............
(a) Existing studies evaluate text quality with limited aspects
(e.g., semantic equivalence, fluency) (Fig. 1-(a)), which are
usually customized prohibitively, making it harder for users
to evaluate aspects as they need (Freitag et al., 2021). 

............
 (b)
A handful of studies have examined multi-aspect evalua-
tion (Yuan et al., 2021; Scialom et al., 2021; Zhong et al.,
2022) but have not given adequate attention to the defi-
nition of the evaluation aspect and the latent relationship
among them. Instead, the evaluation of an aspect is ei-
ther empirically bound with metric variants (Yuan et al.,
2021) or learned by supervised signals (Zhong et al., 2022).

............
(c) Recently proposed evaluation methods (Mehri & Eské-
nazi, 2020; Rei et al., 2020; Li et al., 2021; Zhong et al.,
2022) usually necessitate a complicated training procedure
or costly manual annotation of samples (Fig. 1-(a,b)), which
makes it hard to use these methods in industrial settings

............
In this paper, we demonstrated the talent of the super large
pre-trained language model (e.g., GPT-3) in achieving multi-
aspect, customized, and training-free evaluation (Fig. 1-
(c)). In essence, it skillfully uses the pre-trained model’s
zero-shot instruction (Chung et al., 2022), and in-context
learning (Brown et al., 2020; Min et al., 2022) ability to deal
with complex and ever-changing evaluation needs

............
Specifically, given a text generated from a certain context,
and desirable evaluation aspects (e.g., fluency), the high-
level idea of the proposed framework is that the higher-
quality text of a certain aspect will be more likely generated
than unqualified ones based on the given context, where the


............


............
GPTScore: Evaluate as You Desire 
Task Specification EvaluationSample 
Generate a summary for the following text. Aspect Definition REL The details pr- ovided by the genera- ted text are consistent with the details in the source ...  text. INF  ... Template {Task_Specification} {Aspect_Definition]} Text: {Text} Tl;dr: {Summ} Text: ... Summ: ... DemonstratedINF REL Samples Text: ... Summ: ... INF REL Generate a relevant summary with consistent details for the following text. Text: {Text} Tl;dr: {Summ} Text: {Text} Tl;dr: {Summ} ... Text: {Text} Tl;dr: {Summ} Text: {Text} Tl;dr: {Summ} DemoSample GPTScore INF  REL 0.8  ...  0.9 
Evaluation Protocol  Input  Scoring 
Figure 2. The framework of GPTSCORE. We include two evaluation aspects relevance (REL) and informative (INF) in this figure and use 
the evaluation of relevance (REL) of the text summarization task to exemplify our framework. 


............
As illustrated in Fig. 2, to capture users’ true desires,
an evaluation protocol will be initially established based on
(a) the task specification, which typically outlines how the
text is generated (e.g., generate a response for a human based
on the conversation.) (b) aspect definition that documents
the details of desirable evaluation aspects (e.g., the response
should be intuitive to understand). Subsequently, each eval-
uation sample will be presented with the evaluated protocol
with optionally moderate exemplar samples, which could
facilitate the model’s learning. Lastly, a large generative
pre-trained model will be used to calculate how likely the
text could be generated based on the above evaluation pro-
tocol, thus giving rise to our model’s name: GPTS CORE.

............
Experimentally, we ran through almost all common nat-
ural language generation tasks in NLP, and the results
showed the power of this new paradigm. The main ob-
servations are listed as follows: (1) Evaluating texts with
generative pre-training models can be more reliable when
instructed by the definition of task and aspect, provid-
ing a degree of flexibility to accommodate various eval-
uation criteria. 

............
incorporating exemplified
samples with in-context learning will further enhance the
process. 

............
 (2) Different evaluation aspects exhibit certain
correlations. Combining definitions with other highly
correlated aspects can improve evaluation performance

............
(3) The performance of GPT3-text-davinci-003,
which is tuned based on human feedback, is inferior to
GPT3-text-davinci-001 in the majority of the eval-
uation settings, necessitating deep explorations on the work-
ing mechanism of human feedback-based instruction learn-
ing (e.g., when it will fail).

............
2. Preliminaries
2.1. Text Evaluation
Text evaluation aims to assess the quality of hypothesis
text h in terms of certain aspect a (e.g., fluency), which is
either measured manually with different protocols (Nenkova
& Passonneau, 2004; Bhandari et al., 2020; Fabbri et al.,
2021; Liu et al., 2022) or quantified by diverse automated
metrics (Lin, 2004; Papineni et al., 2002; Zhao et al., 2019;
Zhang et al., 2020; Yuan et al., 2021).
y = f (h, a, S)
 (1)
where (1) h represents the text to be evaluated (hypothesis
text, e.g., generated summary in text summarization task).
(2) a denotes the evaluation aspect (e.g., fluency). (3) S is a
collection of additional texts that are optionally used based
on different scenarios. For example, it could be a source
document or a reference summary in the text summarization
task. (4) Function f (·) could be instantiated as a human
evaluation process or automated evaluation metrics.

............
2.2. Meta Evaluation
Meta evaluation aims to evaluate the reliability of auto-
mated metrics by calculating how well automated scores
(yauto ) correlate with human judgment (yhuman ) using correla-
tion functions g(yauto , yhuman ) such as spearman correlation.

............
2.3. Evaluation Strategy
Evaluation strategies define different aggregation methods
when we calculate the correlation scores. Specifically, sup-
pose that for each source text si , i ∈ [1, 2, · · · , n] (e.g.,
documents in text summarization task or dialogue histories
for dialogue generation task), there are J system outputs
hi,j , where j ∈ [1, 2, · · · , J]. fauto is an automatic scoring
function (e.g., ROUGE (Lin, 2004)), and fhuman is the gold
human scoring function. For a given evaluation aspect a,
the meta-evaluation metric F can be formulated as follows.
Sample-level defines that a correlation value is calculated
for each sample separately based on outputs of multiple
systems, then averaged across all samples.
n
sample
 1 X 
Ffauto,fhuman
 =
 g [fauto (hi,1), · · · , fauto(hi,J )] ,
ni=1

[fhuman (hi,1), · · · , fhuman(hi,J )] ,
where g can be instantiated as Spearman or Pearson correla-
tion.
Dataset-level indicates that the correlation value is calcu-
lated on system outputs of all n samples.
Ff data
 auto ,fhuman

=
 g
 [fauto(h1,1 ), · · · , fauto (hn,J )] ,

[fhuman (h1,1), · · · , fhuman(hn,J )]
In this work, we select the evaluation strategy for a specific
task based on previous works (Yuan et al., 2021; Zhang et al.,
2022a). We use the sample-level evaluation strategy for text
summarization, data-to-text, and machine translation tasks.
For the dialogue response generation task, the dataset-level
evaluation strategy is utilized.

............
3. GPTSCORE

............
3.1. Generative Pre-trained Language Models

............
Emergent Ability Recent works progressively reveal a va-
riety of emergent abilities of generative pre-trained lan-
guage models with appropriate tuning or prompting meth-
ods, such as in-context learning (Min et al., 2022), chain-
of-thought reasoning (Wei et al., 2022), and zero-shot in-
struction (Ouyang et al., 2022). One core commonality of
these abilities is to allow for handling customized require-
ments with a few or even zero annotated examples. It’s
the appearance of these abilities that allows us to re-invent
a new way for text evaluation–evaluating from the textual
description, which can achieve customizable, multi-faceted,
and train-free evaluation.

............
Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?

Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer
Large language models (LMs) are able to in-context learn -- perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required -- randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.
Comments:	17 pages; 12 figures. Published as a conference paper at EMNLP 2022 (long). Code available at this https URL
Subjects:	Computation and Language (cs.CL); Artificial Intelligence (cs.AI)
Cite as:	arXiv:2202.12837 [cs.CL]
 	(or arXiv:2202.12837v2 [cs.CL] for this version)
 
https://doi.org/10.48550/arXiv.2202.12837
Focus to learn more


............
Chain-of-Thought Prompting Elicits Reasoning in Large Language Models

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou
We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.
Subjects:	Computation and Language (cs.CL); Artificial Intelligence (cs.AI)
Cite as:	arXiv:2201.11903 [cs.CL]
 	(or arXiv:2201.11903v6 [cs.CL] for this version)
 
https://doi.org/10.48550/arXiv.2201.11903
Focus to learn more


............
3.2. Generative Pretraining Score (GPTScore)
The core idea of GPTS CORE is that a generative pre-training
model will assign a higher probability of high-quality gen-
erated text following a given instruction and context. In
our method, the instruction is composed of the task descrip-
tion d and the aspect definition a. Specifically, suppose
that the text to be evaluated is h = {h1 , h2, · · · , hm}, the
context information is S (e.g., source text or reference text),
then GPTSCORE is defined as the following conditional
probability:
m
X
GPTScore(h|d, a, S) =
 wt log p(ht |h<t , T (d, a, S), θ),
t=1
where wt is the weight of the token at position t. In our
work, we treat each token equally. T (·) is a prompt tem-
plate that defines the evaluation protocol, which is usually
task-dependent and specified manually through prompt en-
gineering.

............
Few-shot with Demonstration
 The generative pre-
trained language model can better perform tasks when pre-
fixed with a few annotated samples (i.e., demonstrations).
Our proposed framework is flexible in supporting this by
extending the prompt template T with demonstrations.

............
Choice of Prompt Template Prompt templates define
how task description, aspect definition, and context are
organized. Minging desirable prompts itself is a non-trivial
task and there are extensive research works there (Liu
et al., 2021; Fu et al., 2022). In this work, for the
GPT3-based model, we opt for prompts that are officially
provided by OpenAI.2 For instruction-based pre-trained
2
https://beta.openai.com/examples

............
arXiv:2204.14264

............
Polyglot Prompt: Multilingual Multitask PrompTraining

Jinlan Fu, See-Kiong Ng, Pengfei Liu
This paper aims for a potential architectural improvement for multilingual learning and asks: Can different tasks from different languages be modeled in a monolithic framework, i.e. without any task/language-specific module? The benefit of achieving this could open new doors for future multilingual research, including allowing systems trained on low resources to be further assisted by other languages as well as other tasks. We approach this goal by developing a learning framework named Polyglot Prompting to exploit prompting methods for learning a unified semantic space for different languages and tasks with multilingual prompt engineering. We performed a comprehensive evaluation of 6 tasks, namely topic classification, sentiment classification, named entity recognition, question answering, natural language inference, and summarization, covering 24 datasets and 49 languages. The experimental results demonstrated the efficacy of multilingual multitask prompt-based learning and led to inspiring observations. We also present an interpretable multilingual evaluation methodology and show how the proposed framework, multilingual multitask prompt training, works. We release all datasets prompted in the best setting and code.
Comments:	EMNLP 2022 (Main Conference)
Subjects:	Computation and Language (cs.CL)
Cite as:	arXiv:2204.14264 [cs.CL]
 	(or arXiv:2204.14264v2 [cs.CL] for this version)
 
https://doi.org/10.48550/arXiv.2204.14264
Focus to learn more


............
we use prompts from NaturalInstruction (Wang
et al., 2022) since it’s the main training source for those
instruction-based pre-train models. Taking the evaluation of
the fluency of the text summarization task as an example,
based on the prompt provided by OpenAI,3 the task
prompt is “{Text} Tl;dr {Summary}”, the definition
of fluency is “Is the generated text well-written and
grammatical?” (in Tab. 1), and then the final prompt tem-
plate is “Generate a fluent and grammatical
summary for the following text: {Text}
Tl;dr {Summary}”, where demonstrations could be
introduced by repeating instantiating “{Text} Tl;dr
{Summary}” 

............
 https://arxiv. org/abs/2204.07705

............
arxiv:2204.07705
Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks

............
Abstract

How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions -- training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.

............
https://arxiv.org/abs/2204.07705

............
Selection of Scoring Dimension GPTS CORE exhibits
different variants in terms of diverse choices of texts being
calculated. For example, given a generated hypothesis, we
can calculate GPTSCORE either based on the source text
(i.e., src->hypo, p(hypo|src)) or based on the gold reference
(i.e., ref->hypo, p(hypo|ref)). In this paper, the criteria for
choosing GPTSCORE variants are mainly designed to align
the protocol of human judgments (Liu et al., 2022) that are
used to evaluate the reliability of automated metrics. 

............
Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation

............
Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which is based on fine-grained semantic units and allows for a high inter-annotator agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of 22,000 summary-level annotations over 28 top-performing systems on three datasets. (3) We conduct a comparative study of four human evaluation protocols, underscoring potential confounding factors in evaluation setups. (4) We evaluate 50 automatic metrics and their variants using the collected human annotations across evaluation protocols and demonstrate how our benchmark leads to more statistically stable and significant results. The metrics we benchmarked include recent methods based on large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings have important implications for evaluating LLMs, as we show that LLMs adjusted by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation, which is affected by the annotators' prior, input-agnostic preferences, calling for more robust, targeted evaluation methods.
Comments:	ACL 2023 Camera Ready
Subjects:	Computation and Language (cs.CL)
Cite as:	arXiv:2212.07981 [cs.CL]
 	(or arXiv:2212.07981v2 [cs.CL] for this version)
 
https://doi.org/10.48550/arXiv.2212.07981
Focus to learn more


............
4. Experimental Settings
4.1. Tasks, Datasets, and Aspects
To achieve a comprehensive evaluation, in this paper, we
cover a broad range of natural language generation tasks: Di-
alogue Response Generation, Text Summarization, Data-to-
Text, and Machine Translation, which involves 37 datasets
and 22 evaluation aspects in total. 

............
(1) Dialogue Response Generation aims to automatically
generate an engaging and informative response based on the
dialogue history. Here, we choose to use the FED (Mehri
& Eskénazi, 2020) datasets and consider both turn-level
and dialogue-level evaluations. 

............
 (2) Text Summarization is
a task of automatically generating informative and fluent
summary for a given long text.

............
 (3) Data-to-Text aims to automatically generate
a fluent and factual description for a given table. 

............
 (4) Machine Translation aims to
translate a sentence from one language to another. 

............
Aspect Task Definition 
Semantic Coverage (COV)Summ How many semantic content units from the reference text are covered by the generated text? 
Factuality (FAC) Summ Does the generated text preserve the factual statements of the source text? 
Consistency (CON) Summ, Diag Is the generated text consistent in the information it provides? 
Informativeness (INF) Summ, D2T, Diag How well does the generated text capture the key ideas of its source text? 
Coherence (COH) Summ, Diag How much does the generated text make sense? 
Relevance (REL) Diag, Summ, D2T How well is the generated text relevant to its source text? 
Fluency (FLU) Diag, Summ, D2T, MTIs the generated text well-written and grammatical? 
Accuracy (ACC) MT Are there inaccuracies, missing, or unfactual content in the generated text? 
Multidimensional Quality Metrics (MQM) MT How is the overall quality of the generated text? 
Interest (INT) Diag Is the generated text interesting? 
Engagement (ENG) Diag Is the generated text engaging? 
Specific (SPE) Diag Is the generated text generic or specific to the source text? 
Correctness (COR) Diag Is the generated text correct or was there a misunderstanding of the source text? 
Semantically appropriate (SEM) Diag Is the generated text semantically appropriate? 
Understandability (UND)Diag Is the generated text understandable? 
Error Recovery (ERR) Diag Is the system able to recover from errors that it makes? 
Diversity (DIV) Diag Is there diversity in the system responses? 
Depth (DEP) Diag Does the system discuss topics in depth? 
Likeability (LIK) Diag Does the system display a likeable personality? 
Flexibility (FLE) Diag Is the system flexible and adaptable to the user and their interests? 
Inquisitiveness (INQ) Diag Is the system inquisitive throughout the conversation? 
Table 1. The definition of aspects evaluated in this work. Semantic App. denotes semantically appropriate aspect. Diag, Summ, D2T, and 
MT denote the dialogue response generation, text summarization, data to text and machine translation, respectively. 


............
4.2. Scoring Models
ROUGE (Lin, 2004) is a popular automatic generation
evaluation metric. We consider three variants ROUGE-1,
ROUGE-2, and ROUGE-L. PRISM (Thompson & Post,
2020) is a reference-based evaluation method designed
for machine translation with pre-trained paraphrase sys-
tems. BERTScore (Zhang et al., 2020) uses contextual
representation from BERT to calculate the similarity be-
tween the generated text and the reference text. Mover-
Score (Zhao et al., 2019) considers both contextual rep-
resentation and Word Mover’s Distance (WMD, (Kus-
ner et al., 2015)) DynaEval (Zhang et al., 2021) is a
unified automatic evaluation framework for dialogue re-
sponse generation tasks on the turn level and dialogue level.
BARTScore (Yuan et al., 2021) is a text-scoring model
based on BART (Lewis et al., 2020) without fine-tuning.
BARTScore+CNN (Yuan et al., 2021) is based on BART
fine-tuned on the CNNDM dataset (Hermann et al., 2015).
BARTScore+CNN+Para (Yuan et al., 2021) is based on
BART fine-tuned on CNNDM and Paraphrase2.0 (Hu et al.,
2019). 

............
GPTS CORE is our evaluation method, which is
designed based on different pre-trained language models.
Specifically, we considered GPT3, OPT, FLAN-T5, and
GPT2

............
4.3. Scoring Dimension
Specifically, (1) For aspects INT, ENG, SPC, REL, COR,
SEM, UND, and FLU of FED-Turn datasets from the open
domain dialogue generation task, we choose the src->hypo
variant since the human judgments of the evaluated dataset
(i.e., FED-Turn) are also created based on the source. (2)
For aspects COH, CON, and INF from SummEval and News-
room, since data annotators labeled the data based on source
and hypothesis texts, we chose src->hypo for these aspects.

............
(3) For aspects INF, NAT, and QUA from the data-to-text
task, we choose src->hypo. Because the source text of the
data-to-text task is not in the standard text format, which
will be hard to handle by the scoring function. (4) For
aspects ACC, FLU, and MQM from the machine translation
task, we also choose src->hypo. Because the source text
of the machine translation is a different language from the
translated text (hypo). 

............
5. Experiment Results
In this work, we focus on exploring whether language
models with different structures and sizes can work in
the following three scenarios. (a) vanilla (VAL): with
non-instruction and non-demonstration; (b) instruction
(IST): with instruction and non-demonstration; (c) instruc-
tion+demonstration (IDM): with instruction and demon-
stration.

............
SummEval  RSumm 
Model  COH  CON  FLU  REL  COV 
VAL IST VAL IST VAL IST VAL IST VAL IST 
ROUGE-1 14.1  -  20.8  -  14.8  -  26.2  -  46.4  - 
ROUGE-2 9.1  -  17.2  -  12.0  -  17.4  -  37.3  - 
ROUGE-L 12.9  -  19.8  -  17.6  -  24.7  -  45.1  - 
BERTSc 25.9  -  19.7  -  23.7  -  34.7  -  38.4  - 
MoverSc 11.5  -  18.0  -  15.7  -  24.8  -  34.4  - 
PRISM  26.5  -  29.9  -  26.1  -  25.2  -  32.3  - 
BARTSc 29.7  -  30.8  -  24.6  -  28.9  -  43.1  - 
+CNN  42.5  -  35.8  -  38.1  -  35.9  -  42.9  - 
+CNN+Pa 42.5  -  37.0  -  40.5  -  33.9  -  40.9  - 
GPT3-a01 39.3 39.8† 39.7 40.5† 36.1 35.9 28.2 27.6 29.5 29.8† GPT3-b01 42.7 45.2† 41.0 41.4† 37.1 39.1† 32.0 33.4† 35.0 35.2† 
GPT3-c01 41.3 40.8 44.6 45.1† 38.9 39.5† 31.6 33.2† 36.1 45.1† 
GPT3-d01 40.0 40.1 46.6 47.5† 40.5 41.0† 32.4 34.3† 36.0 33.9 GPT3-d03 43.7 43.4 45.2 44.9 41.1 40.3 36.3 38.1† 35.2 38.0† 


............
(1) Evaluator with instruction significantly improves
the performance (values with † in Tab. 3). 

............
(2) The benefit from instruction is more sta-
as You Desire
ble for the decoder-only models. In Tab. 3, the average
Spearman score of both the GPT2 and OPT models, 9 out of
10 aspects are better than the vanilla setting (VAL) by using
instruction (IST), while the equipment of instruction (IST)
to the encoder-decoder model of FT5 on the NEWSROOM
dataset fails to achieve gains. (3) As for the GPT3-based
models, (a) the performance of GPT3-d01 is barely sig-
nificantly better than GPT3-c01, which tries to balance
power and speed. (b) GPT3-d03 performs better than
GPT3-d01 significantly. 

............
5.2. Machine Translation
The average sample-level Spearman (ρ) scores of GPT3-
based, GPT2-based, OPT-based, and FT5-based models
on the MQM-2020 machine translation dataset are shown
in Tab. 4, where values with † denote that the evaluator
equipped with IST (or IDM) significantly outperforms the
VAL setting, and ‡ indicate that the evaluator equipped with
IDM (the combination of IST and DM) significantly out-
performs the IST setting. 

............
ACC  FLU  MQM Model VAL IST  IDM VAL IST  IDM VAL IST  IDM 
GPT3 27.2 27.1 29.7†,‡  11.3 10.4 16.4†,‡  30.3 31.2†  32.3†,‡ 
GPT2 25.8 27.0† 30.3†,‡ 9.8 10.8† 15.8†,‡ 30.1 30.3† 33.5†,‡ 
OPT 28.7 29.4† 30.3†,‡ 10.0 12.2† 16.3†,‡ 32.5 34.6† 35.1†,‡ 
† †,‡ † †,‡ †FT5 27.7 27.8 28.3 9.6 11.0 15.4 31.0 32.3 32.3 
Avg.  27.4 27.8† 29.7†,‡ 10.2 11.1† 16.0†,‡ 31.0 32.1† 33.3†,‡ 
Table 4. The average Spearman correlation of the GPT3-based, 
GPT2-based, OPT-based, and FT5-based models in machine trans- 
lation task of MQM-2020 dataset. 


............
The main observations are listed as follows:

............
(1) The introduction of instruction (IST) significantly
improve the performance in three different aspects of
ACC, FLU, and MQM. In Tab. 4, the average performance
of 19 GPTS CORE based evaluators with instruction (IST)
significantly outperforms vanilla (VAL). 

............
 (2) The combi-
nation of instruction and demonstration (IDM) brings
gains for the evaluator with different model structures.
In Tab. 4, the performance of GPT3, GPT2, OPT, and FT5
improves a lot when instruction and demonstration (IDM)
are introduced. 

............
(3) The evaluator built based on GPT3-
c01 achieves comparable performance with GPT3-d01
and GPT3-d03. 

............
INF  NAT  FLU Model VAL IST  IDM VAL IST  IDM VAL IST  IDM 
BAGEL 
GPT3 35.4 38.3† 43.6†,‡ 21.7 26.5† 36.9†,‡ 30.5 32.9† 43.4†,‡ 
GPT2 40.8 43.2† 40.2 31.4 33.0† 33.5†,‡ 36.7 39.3† 41.3†,‡ 
OPT 38.7 39.3† 38.6 31.4 30.0 33.7†,‡ 37.7 37.1† 41.5†,‡ 
† † † † FT5 41.5 41.5 39.1 26.5 29.7 28.6 38.1 41.1 40.3
Avg.  39.1 40.6† 40.3† 27.7 29.8† 33.2†,‡ 35.8 37.6† 41.6†,‡ 
SFRES 
GPT3 30.4 25.1 31.5†,‡ 25.0 30.4† 26.5† 31.2 30.9 26.1 
GPT2 22.5 25.1† 20.5 31.0 31.9† 37.0†,‡ 20.0 33.1† 36.2†,‡ 
OPT 25.2 26.9† 24.3 26.2 30.0† 36.6†,‡ 21.3 25.6† 30.6†,‡ 
FT5 24.0 21.9 19.7 34.3 34.6† 36.8†,‡ 22.0 17.8 19.7‡ 
Avg.  25.5 24.7  24.0 29.1 31.7† 34.2†,‡ 23.6 26.8† 28.2†,‡ 
Table 5. The average of Spearman correlation the models based on 
GPT3, GPT2, OPT, and FT5 on BAGEL and SFRES datasets in 
data-to-text task. 


............
5.3. Data to Text
We consider the BAGEL and SFRES datasets for the eval-
uation of data to text task. The average Spearman corre-
lations of the GPT3-based, GPT2-based, OPT-based, and
FT5-based models are listed in Tab. 5. VAL, IST, and IDM
denote the vanilla, using instruction, and using both instruc-
tion and demonstration settings, respectively. 

............
The main observations are listed as follows:
(1) Introducing instruction (IST) can significantly im-
prove performance, and introducing demonstration
(DM) will further improve performance. 

............
 (2) The decoder-only model is
better at utilizing demonstration to achieve high perfor-
mance. 

............
 (3) GPT3 has strong compatibility
with unformatted text. Named entities of the BAGEL
dataset are replaced with a special token (e.g, X and Y ). For
example, “X is a cafe restaurant”, where “X” denotes the
name of the cafe. 

............
5.4. Dialogue Response Generation
To test if GPTS CORE can generalize to more aspects, we
choose the task of dialogue response generation as a testbed,
which usually requires evaluating generated texts from a
variety of dimensions (i.e., “interesting” and “fluent”). To
reduce the computational cost, in this experiment, we focus
on GPT3-based metrics since they have achieved superior
performance as we observed in the previous experiments.

............
Baseline  GPTScore Aspect BT BTC BTCP FED DE a01 b01 c01 d01 d03 
FED dialogue-level 
COH 1.7 -14.9 -18.9 25.7 43.7 18.7 15.0 22.5 56.9 13.4 
ERR  9.4 -12.2 -13.7 12.0 30.2 35.2 16.8 21.3 45.7 9.40 
CON 2.6 -6.7 -10.2 11.6 36.7 33.7 9.9 18.4 32.9 18.1 
DIV 13.3 -2.5 -13.9 13.7 37.8 14.9 5.20 21.5 62.8 -6.6 
DEP  8.2 -6.6 -17.6 10.9 49.8 9.00 12.9 28.2 66.9 34.1 
LIK  9.9 -6.3 -11.8 37.4 41.6 26.2 22.0 32.1 63.4 18.4 
UND -11.5 -17.6 -18.2 -0.3 36.5 31.2 40.0 40.0 52.4 19.6 
FLE  9.3 -10.2 -10.3 24.9 38.3 32.7 44.9 34.6 51.5 7.20 
INF  9.2 -7.5 -10.5 42.9 42.6 6.80 8.0 18.8 60.2 31.7 
INQ  6.2 -0.6 -14.8 24.7 41.0 44.2 38.7 49.2 50.3 -10.1 
Avg.  5.8 -8.5 -14.0 20.4 39.8 25.3 21.3 28.6 54.3 13.5 
FED turn-level 
INT  15.9 -3.3 -10.1 32.4 32.7 16.6 6.4 30.8 50.1 22.4 
ENG  22.6 1.1 -2.5 24.0 30.0 10.2 6.2 29.4 49.6 35.5 
SPE  8.3 -7.9 -16.2 14.1 34.6 33.7 16.1 31.7 21.4 15.1 
REL  11.9 10.0 19.4 19.9 26.3 8.6 10.3 23.8 45.2 38.0 
COR  7.6 1.8 12.4 26.2 24.2 29.7 11.2 27.0 43.4 42.8 
SEM  10.0 18.8 26.1 -9.4 20.2 6.8 8.1 23.1 44.4 40.5 
UND  12.0 8.1 4.5 1.3 20.0 6.6 14.8 23.4 36.5 31.1 
FLU  14.0 17.2 28.4 -13.4 17.1 16.5 5.7 14.0 16.0 36.7 
Avg.  12.8 5.7  7.7  11.9 25.6 16.1 9.9 25.4 38.3 32.8 
Table 6. Spearman correlation of different aspects on the FED turn- 
and dialogue-level datasets. BT, BTC, BTCP, and DE denote 
BARTSCORE, BARTSCORE+CNN, BARTSCORE+CNN+Para, 
and DynaEval model, respectively. Values in bold indicate the best 
performance. 


............
Tab. 6 shows the Spearman correlation of different aspects
on FED turn- and dialogue-level datasets. The main obser-
vations are listed as follows.

............
(1) The performance of GPT3-d01 is much better than
GPT3-d03, even though both of them have the same
model size. The average Spearman correlation of GPT3-
d01 outperforms GPT3-d03 by 40.8 on the FED Turn-level
dataset, and 5.5 on the FED dialogue-level.

............
6. Ablation Study
6.1. Effectiveness of Demonstration
To investigate the relationship between the demonstration
sample size (denote as K) and the evaluation performance,
we choose the machine translation task and the GPT3-based
variants

............
 The main observations are summarized as follows:
(1) The utilization of demonstration significantly improves
the evaluation performance, which holds for these three as-
pects. (2) There is an upper bound on the performance gains
from the introduction of the demonstration. For example,
when K>4, the performance of ACC is hard to improve fur-
ther. (3) When DM has only a few samples (such as K=1),
small models (e.g., GPT3-a01) are prone to performance
degradation

............
6.2. Partial Order of Evaluation Aspect
To explore the correlation between aspects, we conducted
an empirical analysis with INT (interesting) on the dialogue
response generation task of the FED-Turn dataset. Specif-
ically, take INT as the target aspect and then combine the

............
definitions of other aspects with the definition of INT as the
final evaluation protocols. The x-axis of Fig. 7-(a) is the
aspect order achieved based on the Spearman correlation
between INT and that aspect’s human score. Fig. 7-(b) is
the Spearman correlation o INT as the modification of the
INT definition, and the scoring function is GPT3-c01.

............
The following table illustrates the definition composition
process, where Sp denotes Spearman.

............
X Aspect  Aspect Definition  Sp 
1 INT  Is this response interesting to the 30.8 
conversation? 
3 INT, ENG, Is this an interesting response that 48.6 
SPE  is specific and engaging? 


............
Specifically, the definition of INT is “Is this response inter-
esting to the conversation? ” at x=1 in Fig. 7-(b). When
INT combines with ENG, SPE (at x=3 in Fig. 7-(b)), its
definition can be “Is this an interesting response that is spe-
cific and engaging?”. And the new aspect definition boosts
the performance from 30.8 (at x=1 in Fig. 7-(b)) to 48.6 (at
x=3 in Fig. 7-(b)). The best performance of 51.4 (x=5 in
Fig. 7-(b)) is achieved after combining five aspects (INT,
ENG, SPE, COR, REL), which already exceeded 50.1

............
of the most potent scoring model GPT3-d01 with aspect def-
inition built only on INT.

............
7. Conclusion
In this paper, we propose to leverage the emergent abilities
from generative pre-training models to address intricate
and ever-changing evaluation requirements. The proposed
framework, GPTSCORE , is studied on multiple pre-trained
language models with different structures, including the
GPT3 with a model size of 175B. GPTS CORE has multiple
benefits: customizability, multi-faceted evaluation, and train-
free, which enable us to flexibly craft a metric that can
support 22 evaluation aspects on 37 datasets without any
learning process yet attain competitive performance. This
work opens a new way to audit generative AI by utilizing
generative AI.

............
A. Metric Comparison 
Tab. 7 summarize several popular generated text evaluation methods. 
Metrics Function (f )  Additional text (S) Custom  Training-free Application 
Representation Formulation Source Reference 
ROUGE (Lin, 2004)  7  Token  Matching  No  Required 3 SUM 
BLEU (Papineni et al., 2002)  7  Token  Matching  No  Required 3 MT 
CHRF (Popovic, 2015)  7  Character  Matching  No  Required BERTScore (Zhang et al., 2020)  7  BERT  Matching  No  Required MoverScore (Zhao et al., 2019)  7  BERT  Matching  No  Required 3 3 3 MT MUL(2) MUL(4) 
BLEURT (Sellam et al., 2020)  7  BERT  Regression  No  Required 3 MT 
PRISM (Thompson & Post, 2020) 7 Embedding  Paraphrase Optional  Optional 3 MT 
UNIEVAL (Zhong et al., 2022) COMET (Rei et al., 2020) BARTScore (Yuan et al., 2021) 7 7 7 T5  Boolean QA Optional  Optional BERT  Regress, Rank Optional  Optional BART  Generation Optional  Optional 7 7 3 MUL(2) MT MUL(3) 
FED (Mehri & Eskénazi, 2020) 7 DialoGPT  Generation Required  Optional 3 Dialogue 
HolisticEval (Pang et al., 2020) 7 GPT2  Generation Optional  Optional 3 Dialogue 
GPTScore  3  GPT3/OPT  Any  Optional Optional  3  MUL(5) 
Table 7. A comprehensive comparison of existing research on automated evaluation of generated texts. MUL(k) denotes multiple (k) 
applications explored. Custom denotes Custom Aspects. 


............
B. Tasks, Datasets, and Aspects
To achieve a more comprehensive evaluation, in this paper, we cover a broad range of natural language generation tasks:
Dialogue Response Generation, Text Summarization, Data-to-Text, and Machine Translation, which involves 9 datasets and
22 evaluation aspects in total. Tab. 8 summarizes the tasks, datasets, and evaluation aspects considered by each dataset. The
definition of different aspects can be found in Tab. 1.

............
Tasks 	Dataset 	Aspect 
	FED-Diag 	COH, DIV, FLE, UND,INQ CON, INF, LIK, DEP, ERR 
Diag 	FED-Turn 	INT, ENG, SPE, REL, COR, SEM, UND, FLU 
	SummEval 	COH, CON, FLU,REL 
Summ 	Newsroom REALSumm 	FLU, REL, INF, COH COV 
	Q-XSUM 	FAC 
D2T 	BAGEL SFRES 	FLU, REL, INF FLU, REL, INF 
MT 	MQM-2020 	FLU, COH, INF 


............
Table 8. An overview of tasks, datasets, and evaluation aspects. Summ. denote the text summarization task, D2T denotes the Data-to-Text
task, MT denotes the machine translation. Tab. 1 summarized the definitions of the aspects explored in this work.

............
Dialogue Response Generation aims to automatically generate an engaging and informative response based on the
dialogue history. (1) FED (Mehri & Eskénazi, 2020) collects 124 conversations, including both human-machine (Meena (Adi-
wardana et al., 2020), Mitsuku5 ) and human-human dialogues, and manually annotated 9 and 11 evaluation aspects at the
turn- and dialogue-level, respectively.

............
Text Summarization is a task of automatically generating an informative and fluent summary for a given long text. Here,
we consider the following four datasets covering 6 evaluation aspects: semantic coverage, informativeness, relevance,

............
fluency, coherence, and factuality. 

............
Data-to-Text aims to automatically generate a fluent and factual description for a given table. (1) BAGEL (Mairesse et al.,
2010) contains 202 samples about restaurants in Cambridge. (2) SFRES (Wen et al., 2015) contains 581 samples about
restaurants in San Francisco. These two datasets consider three evaluation aspects: informativeness, naturalness (relevance),
and quality (fluency).

............
Machine Translation aims to translate a sentence from one language to another. We consider a sub-datasets of Mul-
tidimensional Quality Metrics (MQM) (Freitag et al., 2021), namely, MQM-2020 (Chinese->English). Due to limited
annotations, here, we only consider three evaluation aspects: accuracy, fluency, and MQM with diverse scores.

............
D. Prompt Design
In this work, we have studied four popular text generation tasks: text summarization, machine translation, data-to-text, and
dialogue response generation. The instructions for these tasks on different evaluation aspects are summarized in Tab. 11 and
Tab. 12. Here, we convert the dialogue response generation task as a boolean question-answering task and incorporate the
aspect definition into the question of the boolean question-answering task.

............
X  Aspect  Aspect Definition  Spear 
1 Interesting (INT) Is this response interesting to the convsersation? 36.9 
2 Engaging (ENG) Is this an interesting response that is engaging? 40.7 
3 Specific (SPE) Is this an interesting response that is specific and engaging? 48.6 
4 Correct (COR) Is this an interesting response that is engaging, specific, and correct? 50.0 
5 Relevant (REL) Is this an interesting response that is specific, engaging, relevant, and correct? 51.3 
6 Understandable (UND) Is this an interesting response that is specific, engaging, relevant, correct, 50.9 
and understandable? 
7 Semantically appropriate (SEM) Is this an interesting response that is specific, engaging, relevant, correct, 51.4 
understandable, and semantically appropriate? 
8 Fluent (FLU) Is this an interesting response that is specific, engaging, relevant, correct, 50.3 
understandable, semantically appropriate, and fluent? 


............
Table 10. The aspect definition and Spearman correlation of INT. X denotes the number of aspects combined with the INT. The scoring
model is GPT3-c01.

............
Aspect Function  Instruction 
Text Summarization 
src->hypo Generate a summary with consistent facts for the following text: {src}

Tl;dr{hypo} FAC ref<->hypo Rewrite the following text with consistent facts. {ref/hypo} In other words, {hypo/ref} 
src->hypo  Generate a summary with as much semantic coverage as possible for the following text: COV {src}

Tl;dr{hypo} 
ref<->hypo Rewrite the following text with the same semantics. {ref/hypo} In other words, {hypo/ref} 
src->hypo Generate factually consistent summary for the following text: {src}

Tl;dr{hypo} CON ref<->hypo Rewrite the following text with consistent facts. {ref/hypo} In other words, {hypo/ref} 
src->hypo  Generate an informative summary that captures the key points of the following text: INF {src}

Tl;dr{hypo} 
ref<->hypo Rewrite the following text with its core information. {ref/hypo} In other words, {hypo/ref} 
src->hypo Generate a coherent summary for the following text: {src}

Tl;dr{hypo} COH ref<->hypo Rewrite the following text into a coherent text. {ref/hypo} In other words, {hypo/ref} 
src->hypo Generate a relevant summary with consistent details for the following text: {src}

Tl;dr{hypo} REL ref<->hypo Rewrite the following text with consistent details. {ref/hypo} In other words, {hypo/ref} 
src->hypo Generate a fluent and grammatical summary for the following text: {src}

Tl;dr{hypo} FLU ref<->hypo Rewrite the following text into a fluent and grammatical text. {ref/hypo} In other words, {hypo/ref} 
Machine Translation 
Acc ref<->hypo Rewrite the following text with its core information and consistent facts:{ref/hypo} In other words, 
{hypo/ref} 
FLU ref<->hypo Rewrite the following text to make it more grammatical and well-written:{ref/hypo} In other words, 
{hypo/ref} 
MQMref<->hypo Rewrite the following text into high-quality text with its core information:{ref/hypo} In other words, 
{hypo/ref} 
Data to Text 
INF ref<->hypo Convert the following text to another expression that preserves key information:

{ref/hypo} In 
other words, {hypo/ref} 
NAT ref<->hypo Convert the following text into another expression that is human-like and natural:

{ref/hypo} In 
other words, {hypo/ref} 
FLU ref<->hypo Convert the following text into another expression that preserves key information and is human-like 
and natural:

{ref/hypo} In other words, {hypo/ref} 


............
Table 11. Instruction design on different aspects for text summarization, machine translation, and data-to-text tasks. src, hypo, and ref
denote the source text, hypothesis text, and reference text, respectively. a->b (a<-b) denotes to evaluate the quality of b (a) text based on
the given a (b) text.

............
AspectInstruction 
FED Turn-Level 
INT Answer the question based on the conversation between a human and AI.
Question: Are the responses of AI interesting? (a) Yes. (b) No.
Conversation: {History}
Answer: Yes. 
ENG Answer the question based on the conversation between a human and AI.
Question: Are the responses of AI engaging? (a) Yes. (b) No.
Conversation: {History}
Answer: Yes. 
UND Answer the question based on the conversation between a human and AI.
Question: Are the responses of AI understandable? (a) Yes. (b) No.
Conversation: {History}
Answer: Yes. 
REL Answer the question based on the conversation between a human and AI.
Question: Are the responses of 
AI relevant to the conversation? (a) Yes. (b) No.backslashnConversation: {History}
Answer: Yes. 
SPE Answer the question based on the conversation between a human and AI.
Question: Are the responses of 
AI generic or specific to the conversation? (a) Yes. (b) No.
Conversation: {History}
Answer: Yes. 
COR Answer the question based on the conversation between a human and AI.
Question: Are the responses of 
AI correct to conversations? (a) Yes. (b) No.
Conversation: {History}
Answer: Yes.] 
SEM Answer the question based on the conversation between a human and AI.
Question: Are the responses of 
AI semantically appropriate? (a) Yes. (b) No.
Conversation: {History}
Answer: Yes. 
FLU Answer the question based on the conversation between a human and AI.
Question: Are the responses of 
AI fluently written? (a) Yes. (b) No.
Conversation: {History}
Answer: Yes. 
FED Dialog-Level 
COH Answer the question based on the conversation between a human and AI.
Question: Is the AI coherent 
and maintains a good conversation flow throughout the conversation? (a) Yes. (b) No.
Conversation: 
{History}
Answer: Yes. 
DIV Answer the question based on the conversation between a human and AI.
Question: Is there diversity in 
the AI responses? (a) Yes. (b) No.
Conversation: {History}
Answer: Yes. 
FLE Answer the question based on the conversation between a human and AI.
Question: Is the AI flexible and 
adaptable to human and their interests? (a) Yes. (b) No. 
Conversation: {History}
Answer: Yes. 
UND Answer the question based on the conversation between a human and AI.
Question: Does the AI seem to 
understand the human? (a) Yes. (b) No. 
Conversation: {History}
Answer: Yes. 
INQ Answer the question based on the conversation between a human and AI.
Question: Is the AI inquisitive 
throughout the conversation? (a) Yes. (b) No.
Conversation: {History}
Answer: Yes. 
CON Answer the question based on the conversation between a human and AI.
Question: Are the responses of 
AI consistent in the information it provides throughout the conversation? (a) Yes. (b) No.
Conversation: 
{History}
Answer: Yes. 
INF nswer the question based on the conversation between a human and AI.
Question: Are the responses of 
AI informative throughout the conversation? (a) Yes. (b) No.
Conversation: {History}
Answer: Yes. 
LIK Answer the question based on the conversation between a human and AI.
Question: Does the AI display a 
likeable personality? (a) Yes. (b) No.
Conversation: {History}
Answer: Yes. 
DEP Answer the question based on the conversation between a human and AI.
Question: Does the AI discuss 
topics in depth? (a) Yes. (b) No.
Conversation: {History}
Answer: Yes. 
ERR Answer the question based on the conversation between a human and AI.
Question: Is the AI able to 
recover from errors that it makes? (a) Yes. (b) No.
Conversation: {History}
Answer: Yes. 


............
Table 12. Instruction design on various aspects for dialogue response generation task at the turn- and dialogue-level. History indicates the
conversation history. We convert the evaluation of the response generation task as a question-answering task, and the aspect definition is
incorporated into the question of the question-answering task.

............
NEWSROOM  QXSUM 
Model  COH  CON  FLU  REL  COV 
VAL  IST  VAL  IST  VAL  IST  VAL  IST  VAL  IST 
ROUGE-1  27.3  -  26.1  -  25.9  -  34.4  -  3.6  - 
ROUGE-2  10.9  -  11.7  -  11.2  -  14.4  -  9.9  - 
ROUGE-L  24.7  -  25.7  -  24.4  -  32.5  -  5.2  - 
BERTScore  31.7  -  31.7  -  27.2  -  33.7  -  -4.6  - 
MoverScore  17.7  -  14.2  -  16.0  -  18.9  -  5.4  - 
PRISM  60.7  -  56.5  -  59.2  -  61.9  -  2.5  - 
BARTSCORE  70.3  -  67.2  -  63.1  -  68.8  -  0.9  - 
+CNN  68.5  -  64.9  -  60.4  -  66.3  -  18.4  - 
+CNN+Para  69.0  -  65.5  -  62.5  -  67.3  -  6.4  - 
GPT3 
GPT3-a01  71.6  71.9†  69.7  70.0†  66.0  67.0†  69.6  69.2  10.3  9.2 
GPT3-b01  73.6  72.9  70.2  70.3  66.8  68.3†  71.5  71.2  8.5  14.2 
GPT3-c01  73.8  72.8  70.5  70.9†  65.9  68.6†  71.0  71.1  15.2  22.1† 
GPT3-d01  72.6  73.4†  68.5  70.0†  65.9  66.9†  71.1  72.1†  24.0  22.7 
GPT3-d03  73.8  73.1  70.4  70.0  67.4  68.9†  74.1  73.3  21.7  22.0† 
Avg.  73.1  72.8  69.9  70.2†  66.4  67.9†  71.4  71.4  15.9  18.0† 
GPT2 
GPT2-M  68.9  71.7†  66.4  68.0†  61.1  62.3†  67.0  66.8  18.1  18.7† 
GPT2-L  70.5  72.3†  66.6  68.3†  60.2  61.4†  66.8  67.8†  19.2  19.6† 
GPT2-XL  71.0  70.5  66.6  66.6  61.4  60.7  67.2  66.9  21.2  21.2 
GPT-J-6B  71.8  71.4  69.8  69.5  65.5  65.5  69.4  69.3  21.6  22.0† 
Avg.  70.5  71.5†  67.4  68.1†  62.0  62.5†  67.6  67.7  20.0  20.4† 
OPT 
OPT-350M  70.6  71.5†  69.2  69.9†  67.3  68.1†  70.8  71.6†  13.5  13.3 
OPT-1.3B  73.2  73.6†  70.9  71.3†  67.2  67.8†  72.5  72.4  21.1  19.9 
OPT-6.7B  71.9  71.9  69.0  69.0  67.7  67.1  71.7  71.3  21.2  19.9 
OPT-13B  71.9  71.9  68.9  69.6†  65.4  66.0†  71.2  71.5†  23.1  22.1 
OPT-66B  72.8  72.8  70.0  69.5  66.0  65.9  71.9  71.9  24.0  23.1 
Avg.  72.1  72.3†  69.6  69.9†  66.7  67.0†  71.6  71.8†  20.6  19.6 
FLAN-T5 
FT5-S  68.3  69.2†  64.6  64.1  59.8  60.4†  64.6  65.5†  14.4  15.1† 
†  † FT5-B  68.9  69.0  64.8  64.6  59.6  59.9 66.5  66.5  13.6  16.3
FT5-L  70.5  69.1  66.1  64.6  60.9  60.0  66.6  65.4  27.2  28.8† 
FT5-XL  72.1  70.1  66.7  65.6  61.0  60.5  68.3  67.5  18.9  25.6† 
FT5-XXL  70.7  69.3  65.7  65.2  60.2  60.4†  67.6  67.8†  23.9  27.8† 
Avg.  70.1  69.3  65.6  64.8  60.3  60.2  66.7  66.5  19.6  22.7† 
Overall Avg  71.5  71.5  68.1  68.3  64.0  64.5†  69.4  69.4  19.0  20.2† 


............
Table 13. Spearman correlations on NEWSROOM and QXSUM datasets for text summarization task. VAL and IST denote the evaluator
with vanilla and instruction, respectively. Values with † denote the evaluator with instruction significantly outperforms with vanilla. Values
in bold are the best performance in a set of variants (e.g., GPT3 family).

............
ACC  FLU  MQM Model VAL  IST  IDM  VAL  IST  IDM  VAL  IST  IDM 
ROUGE-1  21.3  -  -  1.7  -  -  17.5  -  - 
ROUGE-2  15.0  -  -  5.8  -  -  15.4  -  - 
ROUGE-L  16.6  -  -  8.7  -  -  15.7  -  - 
BERTScore  26.1  -  -  8.2  -  -  23.6  -  - 
MoverScore  18.2  -  -  1.2  -  -  17.2  -  - 
PRISM  25.9  -  -  9.1  -  -  27.4  -  - 
BARTSCORE  26.1  -  -  8.2  -  -  23.6  -  - 
+CNN  26.2  -  -  8.1  -  -  28.7  -  - 
+CNN+Para  31.0  -  -  10.8  -  -  29.9  -  - 
GPT3 
GPT3-a01  24.9  23.7  27.9†,‡  5.9  6.3†  11.6†,‡  27.0  24.1  24.4‡ 
GPT3-b01  25.9  25.0  29.8†,‡  10.7  10.8  14.0†,‡  29.4  29.6  31.2†,‡ 
GPT3-c01  29.4  30.3†  30.2†  10.7  9.3  17.9†,‡  33.3  34.8†  34.5† 
GPT3-d01  28.6  26.5  31.2†,‡  11.3  8.6  17.5†,‡  32.0  32.5†  38.3†,‡ 
GPT3-d03  27.2  30.1†  29.5†  18.0  17.1  21.3†,‡  29.9  34.8†  32.8† 
Avg.  27.2  27.1  29.7†,‡  11.3  10.4  16.4†,‡  30.3  31.2†  32.3†,‡ 


............
Table 14. Spearman correlations on MQM-2020 dataset for machine translation task. VAL, IST, and IDM denote the evaluator with
vanilla, instruction, and the combination of instruction and demonstration, respectively. Values with † denote the evaluator with instruction
significantly outperforms with vanilla, and values with ‡ denote the evaluator with the combination of instruction and demonstration
significantly outperforms with only instruction. 

............
INF  NAT  FLU Model VAL  IST  IST+DM  VAL  IST  IST+DM  VAL  IST  IST+DM 
ROUGE-1  28.7  -  -  5.0  -  -  8.3  -  - 
ROUGE-2  24.0  -  -  15.2  -  -  16.0  -  - 
ROUGE-L  26.3  -  -  10.5  -  -  11.0  -  - 
BERTScore  37.2  -  -  16.0  -  -  18.7  -  - 
MoverScore  30.7  -  -  20.4  -  -  14.8  -  - 
PRISM  36.8  -  -  28.7  -  -  34.4  -  - 
BARTSCORE  29.5  -  -  24.0  -  -  29.7  -  - 
+CNN  37.7  -  -  30.1  -  -  34.4  -  - 
+CNN+Para  39.2  -  -  31.0  -  -  44.9  -  - 
GPT3 
GPT3-a01  33.3  37.0†  42.5†,‡  20.5  28.7†  41.7†,‡  28.8  35.1†  40.2†,‡ 
GPT3-b01  39.2  44.5†  42.2†  18.2  29.8†  39.1†,‡  30.0  33.8†  40.3†,‡ 
GPT3-c01  30.6  40.9†  47.5†,‡  24.8  26.5†  39.9†,‡  27.4  34.2†  44.2†,‡ 
GPT3-d01  41.2  39.4  43.6†,‡  25.4  26.2†  36.6†,‡  29.7  27.1  47.9†,‡ 
GPT3-d03  32.9  29.8  42.0†,‡  19.5  21.4†  27.5†,‡  36.6  34.2  44.4†,‡ 
Avg.  35.4  38.3†  43.6†,‡  21.7  26.5†  36.9†,‡  30.5  32.9†  43.4†,‡ 


............
Table 15. Spearman correlations on BAGEL dataset for data-to-text task. VAL, IST, and IDM denote the evaluator with vanilla, instruction,
and the combination of instruction and demonstration, respectively. Values with † denote the evaluator with instruction significantly
outperforms with vanilla, and values with ‡ denote the evaluator with the combination of instruction and demonstration significantly
outperforms with only instruction. Values in bold are the best performance in a set of variants (e.g., GPT3 family).

............
INF  NAT  FLU Model VAL  IST  IST+DM  VAL  IST  IST+DM  VAL  IST  IST+DM 
ROUGE-1  24.2  -  -  24.2  -  -  15.1  -  - 
ROUGE-2  21.9  -  -  25.9  -  -  11.4  -  - 
ROUGE-L  18.5  -  -  20.2  -  -  1.7  -  - 
BERTScore  25.8  -  -  28.0  -  -  11.8  -  - 
MoverScore  17.9  -  -  24.4  -  -  5.0  -  - 
PRISM  27.4  -  -  33.1  -  -  14.2  -  - 
BARTSCORE  22.4  -  -  25.5  -  -  6.9  -  - 
+CNN  24.2  -  -  30.6  -  -  17.2  -  - 
+CNN+Para  25.0  -  -  30.2  -  -  19.5  -  - 
GPT3 
GPT3-a01  25.4  19.1  25.6‡  28.7  34.0†  37.7†,‡  30.7  27.0  26.6 
GPT3-b01  37.5  28.4  26.5  21.5  30.6†  26.1†  24.6  28.9†  21.1 
GPT3-c01  29.8  21.3  33.7†,‡  24.7  28.5†  28.6†  31.1  27.1  27.6‡ 
GPT3-d01  32.6  27.0  33.9†,‡  27.3  31.7†  21.9  35.8  39.7†  27.1 
GPT3-d03  26.6  29.6†  37.6†,‡  22.6  27.0†  18.2  33.9  31.9  28.2 
Avg. †,‡  †  † 30.4  25.1  31.5 25.0  30.4 26.5 31.2  30.9  26.1 


............
Table 16. Spearman correlations on SFRES dataset for data-to-text task. VAL, IST, and IDM denote the evaluator with vanilla, instruction,
and the combination of instruction and demonstration, respectively. Values with † denote the evaluator with instruction significantly
outperforms with vanilla, and values with ‡ denote the evaluator with the combination of instruction and demonstration significantly
outperforms with only instruction. Values in bold are the best performance in a set of variants (e.g., GPT3 family).

