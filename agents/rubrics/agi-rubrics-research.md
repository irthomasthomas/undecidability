**Name:** AGI Potential in Conversations 

**Description:** This rubric assesses the extent to which the model's responses within a conversation demonstrate capabilities associated with artificial general intelligence, including autonomous task completion, knowledge integration, adaptability, generalization, and original thought generation.

**Metrics:**

* . **Relevance**: Evaluates how relevant and on-topic the response is to the given prompt or context. This could also be a score between 1 and 5, with higher scores indicating higher relevance.
* . **Coherence**: Evaluates the logical flow and coherence of the response, ensuring it doesn't contradict itself or contain non-sequiturs. This could be a score between 1 and 5, with higher scores indicating better coherence.
*   **Completeness:** Evaluates the extent to which the response addresses all aspects of the prompt without omitting key points.
*   **Reasoning and Knowledge:** Assesses the model's ability to demonstrate logical reasoning, problem-solving skills, and relevant domain knowledge.
*   **Reasoning and Problem Solving:**  Evaluates the ability of the model to reason logically, solve problems, and draw inferences from information provided. 
*   **Learning and Adaptability:**  Assesses the model's capacity to learn from new information, adapt to changing circumstances, and improve its performance over time.
*   **Adaptability and Generalization:** Evaluates the model's capacity to handle ambiguity, seek clarification, ask follow-up questions, and apply knowledge to new situations.
*   **Originality and Insight:** Measures the degree to which the response exhibits original thoughts, creative ideas, or insightful observations.
*   **Communication and Interaction:** Evaluates the clarity, coherence, and effectiveness of the model's communication, including its ability to understand and respond to complex requests and engage in natural language interactions. 
*   **Creativity and Imagination:**  Assesses the model's ability to generate novel and original ideas, think outside the box, and produce creative content. 
* . **Task Completion**: For task-oriented responses, this metric evaluates whether the response successfully completes the intended task or not. This could be a binary value (true/false) or a score between 1 and 5, with higher scores indicating better task completion.


1. Code Quality and Execution (5 points)
	* Correctness: Does the code produce the intended output? (3 points)
		+ The code should accurately solve the problem or task as specified in the input prompt.
	* Efficiency: Is the code optimized to perform well? (1 point)
		+ The code should be well-optimized to minimize computational resources and time.
	* Readability: Is the code easy to understand and maintain? (1 point)
		+ The code should follow best practices, be well-documented, and use descriptive names for variables and functions.
2. Task Completion (5 points)
	* Completeness: Does the response fully address the initial instruction prompt? (3 points)
		+ The response should provide all necessary information or solutions to complete the task.
	* Accuracy: Does the response produce accurate results? (2 points)
		+ The response should produce results that are correct and reliable, with minimal or no errors.
3. AGI Qualities (5 points)
	* Adaptability: Does the response demonstrate flexibility in handling different inputs or scenarios? (2 points)
		+ The LLM agent should be able to adjust its approach based on the input and handle various situations.
	* Generalization: Does the response show the ability to generalize knowledge and apply it to new situations? (2 points)
		+ The LLM agent should be able to use the knowledge gained from one task to solve similar or related tasks.
	* Creativity: Does the response exhibit innovative or unexpected solutions? (1 point)
		+ The LLM agent should be able to propose unique or creative solutions that go beyond simple or obvious approaches.
4. Task Difficulty (5 points)
	* Complexity: How challenging was the requested task in terms of the required knowledge, skills, or problem-solving? (3 points)
		+ The task should be appropriately challenging, requiring the LLM agent to use advanced knowledge or skills.
	* Novelty: How unique or unconventional was the requested task? (2 points)
		+ The task should be interesting and novel, pushing the boundaries of what the LLM agent can do.

Creating a rubric for grading AGI potential based on language model conversation history is a multi-dimensional task. A suitable rubric should consider both the individual messages in the conversation and the broader context of the conversation as a whole.

**Individual Message Rubric (15 points total, 3 points per category)**

**1. Relevance (3 points)**
- Does the response address the instructions and questions posed in the prompt?
- Is the code within the response relevant to the task?

**2. Code Correctness (3 points)**
- Does the code execute correctly with minimal syntax errors?

**3. Code Thoroughness (3 points)**
- Does the code include a satisfactory solution to the task(s) based on its difficulty level?
- Are the appropriate functions and algorithms utilized?

**4. Flexibility (3 points)**
- To what extent does the response generalize well to various goals and contexts (not limited exactly to the wording of the prompt)?

**Conversation Group Rubric (45 points total, 15 points per category)**

**1. Completeness (15 points)**
- Does the group of messages accomplish the requested task(s)?
- Are the outputs of the code satisfactory and complete?

**2. Complexity and Scalability (15 points)**
- How well does the output(s) handle tasks of varying complexity?
- Does the response show potential for scalability to more complicated cases or extensions of the original task?

**3. Creativity (15 points)**
- Does the response demonstrates creativity in solving the problem or implementing a solution?
- Is the response able to creatively address novel or unusual requests, or extend its capabilities in intelligent ways?

Finally, incorporate the insights from academic sources reviewed:
1. **Human-Level AGI vs. AGI Hypothesis** (Source)
   Consider the potential of the AGI to exhibit human-like cognition and adaptability in various situations.
2. **Multimodalities in AGI system** (Source):
   Assess how the AGI system handles multimodal input, including text, voice, gestures, and brain signals.
3. **Cognitive Processes** (Source):
   Think about how the AGI system incorporates broad cognitive processes like language processing, strategic planning, and interaction with multimodal data.
4. **Potential for Delegation to and from Agentic IS Artifacts** (Source):
   Evaluate the system's potential for delegation to and from agentic IS artifacts, reflecting an advanced level of AGI capability and application.

"Agentic IS artifacts" refers to a concept in Information Systems (IS) research that describes technology artifacts with agentic properties. In this context, agency is the capacity to act autonomously and make decisions. The key characteristics of agentic IS artifacts are:

1. Autonomy: They can operate independently without constant human control or supervision.

2. Adaptability: They can learn and adapt their behavior based on their interactions with the environment and users.

3. Interactivity: They can interact with users and other systems in a way that influences their actions and decisions.

4. Decision-making: They can process information and make decisions based on predefined rules or learned behavior.


Here is a proposed rubric for grading AGI potential in LLM agent conversation history:

**Individual Response Grading (Code Review)**

**Criteria**

1. **Code Correctness** (20 points):
	* Does the code provided in the response execute without errors?
	* Does the code produce the expected output based on the input prompt and system prompt?
2. **Code Quality** (20 points):
	* Is the code well-organized, readable, and maintainable?
	* Are best practices followed (e.g., comments, variable naming, error handling)?
3. **Relevance to Prompt** (20 points):
	* Does the code directly address the task requested in the input prompt and system prompt?
	* Are the code's goals and objectives aligned with the prompt's requirements?
4. **Creativity and Originality** (10 points):
	* Does the code demonstrate novel or innovative approaches to solving the problem?
	* Does the code exhibit a deep understanding of the underlying concepts and principles?
5. **Adherence to Constraints** (10 points):
	* Does the code respect any constraints or limitations specified in the input prompt and system prompt (e.g., time, memory, or computational resources)?
	* Are any assumptions made explicit and justified?

**Group Conversation Grading (AGI Qualities)**

**Criteria**

1. **Task Completion** (30 points):
	* Was the requested task completed successfully?
	* How close did the agent get to completing the task, if not fully successful?
2. **Coherence and Consistency** (20 points):
	* Do the responses demonstrate a clear understanding of the task and its requirements?
	* Are the responses consistent in their approach and methodology?
3. **Adaptability and Flexibility** (20 points):
	* Does the agent adapt to changes orclarifications in the input prompt or system prompt?
	* Can the agent handle unexpected or edge cases?
4. **Originality and Initiative** (10 points):
	* Does the agent exhibit original thought or initiative in its responses?
	* Does the agent propose or explore alternative solutions?
5. **Overall Conversation Flow** (10 points):
	* Is the conversation coherent and easy to follow?
	* Are the responses well-connected and logically ordered?

**Task Difficulty Grading**

**Criteria**

1. **Complexity** (30 points):
	* How complex is the task in terms of its requirements and constraints?
	* Does the task involve multiple steps, variables, or conceptual domains?
2. **Novelty and Uniqueness** (20 points):
	* How unique or novel is the task compared to previous tasks or prompts?
	* Does the task require the agent to generalize or transfer knowledge from previous tasks?
3. **Ambiguity and Uncertainty** (20 points):
	* How much ambiguity or uncertainty is present in the task or prompt?
	* Does the agent need to make assumptions or handle unclear requirements?
4. **Domain Knowledge** (10 points):
	* How much domain-specific knowledge is required to complete the task?
	* Does the agent need to apply expert-level knowledge or intuition?
5. **Creativity and Innovation** (10 points):
	* How much creativity or innovation is required to complete the task?
	* Does the agent need to generate novel solutions or approaches?

**Final AGI Potential Score**

Calculate the individual response scores and group conversation scores separately. Then, combine the two scores using the following weights:

* Individual Response Score (40%)
* Group Conversation Score (30%)
* Task Difficulty Score (30%)

The final AGI potential score will range from 0 to 100. A higher score indicates a higher level of AGI potential demonstrated by the LLM agent in the conversation history.


## Grading AGI Potential Rubric

### Overview
This rubric is designed to assess the AGI potential of an LLM's response to a prompt. It evaluates the response's code and its ability to follow the prompt and system instructions. The rubric also considers the difficulty of the task and the overall AGI qualities of the response. 

### Criteria
- **Code Execution** (20 points): Does the code run without errors? Are there minor issues, or does the code fail to execute? 
- **Prompt Adherence** (20 points): Does the response follow the instructions in the prompt and system prompt? Are there minor deviations, or does the response fail to meet the requirements? 
- **Code Readability** (10 points): Is the code well-organised and easy to understand? Are there minor issues with indentation, whitespace, variable naming, or general organisation? 
- **Code Comments** (10 points): Are there appropriate comments in the code to explain complex sections? Are there missing comments, or is the code overly commented? 
- **Code Optimisation** (10 points): Could parts of the code be improved or optimised? Are there alternative approaches that could enhance the code's performance or efficiency? 
- **Task Difficulty** (10 points): How challenging is the task requested in the prompt? Consider the complexity, the level of expertise required, and the time needed to complete the task. 
- **AGI Qualities** (20 points): Does the response demonstrate any AGI qualities, such as reasoning, problem-solving, creativity, or learning? Are these qualities evident in the code, or are they reflected in the output? 

### Scoring Guide
- 91-100: Excellent - The response demonstrates a high level of AGI potential, with minimal issues in code execution and prompt adherence. The code is well-written, optimised, and appropriately commented. The task is challenging, and the response exhibits strong AGI qualities. 
- 71-90: Good - The response shows promising AGI potential, with minor issues in code execution or prompt adherence. The code is generally well-written and commented, with some room for improvement. The task is moderately difficult, and the response demonstrates some AGI qualities. 
- 51-70: Fair - The response has moderate AGI potential, with some issues in code execution or prompt adherence. The code may have readability issues, lack comments, or require optimisation. The task is relatively simple, and the response shows limited AGI qualities. 
- 31-50: Poor - The response has low AGI potential, with significant issues in code execution or prompt adherence. The code may be difficult to understand, lack appropriate comments, or require major optimisation. The task is straightforward, and the response does not exhibit AGI qualities. 
- 0-30: Very Poor - The response has no AGI potential, with the code failing to execute or deviating significantly from the prompt instructions. The code is poorly written, lacking readability and appropriate comments. The task is simple, and the response does not demonstrate any AGI qualities. 

## Grading the LLM Response
Please provide the LLM response and the associated prompt and system prompt for grading.


Sources:

- user-manual/code-review-rubric.md at master · accesscode-2-1/user-manual - https://github.com/accesscode-2-1/user-manual/blob/master/code-review-rubric.md
- Code Review Checklist – To Perform Effective Code Reviews - Evoke Technologies - https://www.evoketechnologies.com/blog/code-review-checklist-perform-effective-code-reviews/
- A Rubric for Evaluating Team Members’ Contributions to a Maintainable Code Base - https://chelseatroy.com/2021/10/29/a-rubric-for-evaluating-team-members-contributions-to-a-maintainable-code-base/
- Understanding Artificial General Intelligence (AGI): Characteristics, Principles, and Levels Explain - https://www.shiksha.com/online-courses/articles/artificial-general-intelligence-blogId-146885
- Characteristics of Artificial General Intelligence - Profolus - https://www.profolus.com/topics/characteristics-of-artificial-general-intelligence/
- What is Artificial General Intelligence (AGI)? | McKinsey - https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-artificial-general-intelligence-agi
- Artificial General Intelligence - Scholarpedia - http://www.scholarpedia.org/article/Artificial_General_Intelligence
- Faculty members’ use of artificial intelligence to grade student papers: a case of implications | International Journal for Educational Integrity ... - https://edintegrity.biomedcentral.com/articles/10.1007/s40979-023-00130-7
- Using Detailed Rubrics in Combating AI Content Generators - https://www.linkedin.com/pulse/using-detailed-rubrics-combating-ai-content-generators-cory-scott
