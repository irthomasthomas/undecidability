**Prompt:**
title:	Search conversation topics and continue
state:	OPEN
author:	irthomasthomas
labels:	
comments:	1
assignees:	
projects:	
milestone:	
number:	4
--
I want to quickly search my conversation history by topic and select conversations to continue.

Create vector embeddings of conversations.
Search topics
Continue conversations

-i interactive mode with Terminal menu to select conversations
 Which tools linked in the article are actually relevant to the issue? Only mention tools with a url. Keep answer short and concise. Article: null
"<div id=\"readability-page-1\" class=\"page\"><div>\n\n\n\n<p>29th April 2023</p>\n\n<p>I shipped <a href=\"https://github.com/simonw/openai-to-sqlite/releases/tag/0.3\">openai-to-sqlite 0.3</a> yesterday with a fun new feature: you can now use the command-line tool to enrich data in a SQLite database by running values through an OpenAI model and saving the results, all in a single SQL query.</p>\n<p>The idea for this came out of a conversation in a <a href=\"https://calendly.com/swillison/datasette-office-hours\">Datasette Office Hours</a> session. I was asked if there was a way to do sentiment analysis using Datasette. There isn’t, yet—and the feature I’ve been planning that would enable that (which I’m calling “enrichments”) is still a fair way out.</p>\n<p>But it got me to thinking... what’s the simplest possible way to run sentiment analysis on a column of data in a SQLite database using the tools I’ve already built?</p>\n<p>I ended up adding a small new feature to my <a href=\"https://datasette.io/tools/openai-to-sqlite\">openai-to-sqlite</a> tool: the ability to call the OpenAI API (currently just the ChatGPT / <code>gpt-3.5-turbo</code> model) with a SQL query, plus a new <code>chatgpt(prompt)</code> SQL function for executing prompts.</p>\n<p>This means you can do sentiment analysis something like this:</p>\n<div><pre>openai-to-sqlite query database.db <span><span>\"</span></span>\n<span>  update messages set sentiment = chatgpt(</span>\n<span>    'Sentiment analysis for this message: ' || message ||</span>\n<span>    ' - ONLY return a lowercase string from: positive, negative, neutral, unknown'</span>\n<span>  )</span>\n<span>  where sentiment not in ('positive', 'negative', 'neutral', 'unknown')</span>\n<span>    or sentiment is null</span>\n<span><span>\"</span></span></pre></div>\n<p>Running this command causes the <code>sentiment</code> column on the <code>messages</code> table to be populated with one of the following values: <code>positive</code>, <code>negative</code>, <code>neutral</code> or <code>unknown</code>.</p>\n<p>It also prints out a cost estimate at the end. To run against 400 rows of data (each the length of a group chat message, so pretty short) cost me 20,000 tokens, which was about 4 cents. <code>gpt-3.5-turbo</code> is <em>cheap</em>.</p>\n<p>The command uses an OpenAI API key from the <code>OPENAI_API_KEY</code> environment variable, or you can pass it in using the <code>--token</code> option to the command.</p>\n<p>The tool also displays a progress bar while it’s running, which looks like this:</p>\n<p><img src=\"https://static.simonwillison.net/static/2023/openai-to-sqlite-progress.gif\" alt=\"Terminal showing that full command, plus a progress bar that counts from 46/413 up to 44/413 and shows the estimated time remaining\"></p>\n<h4>Sentiment analysis with ChatGPT</h4>\n<p>Here’s the SQL query that I ran, with extra inline comments:</p>\n<div><pre><span>update</span> messages\n  <span><span>--</span> we're updating rows in the messages table</span>\n  <span>set</span> sentiment <span>=</span> chatgpt(\n    <span><span>--</span> Construct the ChatGPT prompt</span>\n    <span><span>'</span>Sentiment analysis for this message: <span>'</span></span> <span>||</span>\n    message <span>||</span>\n    <span><span>'</span> - ONLY return a lowercase string from:<span>'</span></span> <span>||</span>\n    <span><span>'</span>positive, negative, neutral, unknown<span>'</span></span>\n)\n<span>where</span>\n  <span><span>--</span> Don't update rows that already have a sentiment</span>\n  sentiment not <span>in</span> (\n    <span><span>'</span>positive<span>'</span></span>, <span><span>'</span>negative<span>'</span></span>, <span><span>'</span>neutral<span>'</span></span>, <span><span>'</span>unknown<span>'</span></span>\n  ) <span>or</span> sentiment is <span>null</span></pre></div>\n<p>And here’s the prompt I’m using:</p>\n<blockquote>\n<p>Sentiment analysis for this message: {message}—ONLY return a lowercase string from: positive, negative, neutral, unknown</p>\n</blockquote>\n<p>As usual with prompt engineering, you end up having to practically BEG the model to stick to the rules. My first version of this prompt produced all kinds of unexpected output—this version mostly does what I want, but still ends up spitting out the occasional <code>Positive.</code> or <code>Sentiment: Negative</code> result despite my pleas for just those four strings.</p>\n<p>I’m sure there are better prompts for this. I’d love to see what they are!</p>\n<p>(The most likely improvement would be to include some examples in the prompt, to help nail down the expected outputs—an example of few-shot training.)</p>\n<h4>Running prompts with a SELECT</h4>\n<p>I have <a href=\"https://simonwillison.net/2023/Apr/4/llm/\">other tools</a> for running prompts from the command-line, but if you want to use this to execute a prompt directly without writing to a database you can execute against the <code>:memory:</code> SQLite in-memory database like this:</p>\n<pre><code>openai-to-sqlite query :memory: \\\n  \"select chatgpt('Surprising names for a horse')\"\n</code></pre>\n<p><img src=\"https://static.simonwillison.net/static/2023/openai-to-sqlite-query.jpg\" alt=\"Running that command produces JSON output with 1. Marshmallow 2. Chewbacca 3. Rainbow 4. Disco Ball 5. Gummy Bear 6. Sparkle 7. Pudding 8. Twinkle Toes 9. Lightning Bolt 10. Bubblegum - and a total price of $0.0001 (0.0136 cents)\"></p>\n<h4>How the progress bar works</h4>\n<p>When I first implemented this, it didn’t have a progress bar.</p>\n<p>This turned out to be a pretty big problem!</p>\n<p>A SQL update that affects 400 rows, each one involving an API call to OpenAI, can take a few minutes to run.</p>\n<p>During that time, there is no feedback at all to show you that it’s working, or indicate how far it’s gone.</p>\n<p>The <code>UPDATE</code> statement itself runs inside a transaction, so you can’t even peak in the database to see how it’s going—all 400 rows will appear once, at the end of the query.</p>\n<p>I really wanted a progress bar. But how could I implement that? I need it to update as the query progresses, and I also need to know how many API calls it’s going to make in advance in order to correctly display it.</p>\n<p>I figured out a neat way of doing this.</p>\n<p>The trick is to run the SQL query twice. The first time, implement a dummy <code>chatgpt()</code> function that counts how many times it has been called but doesn’t make an API call.</p>\n<p>Then, at the end of that query... rollback the transaction! This means that the dummy <code>chatgpt()</code> function will have been called the correct number of times, but the database will be left unchanged.</p>\n<p>Now register the “real” <code>chatgpt()</code> function and run the query a second time.</p>\n<p>That second <code>chatgpt()</code> Python function can increment the counter used by the progress bar each time it’s called.</p>\n<p>The result is a progress bar that knows how many API calls are going to be made and updates as the query progresses.</p>\n<p>Here’s <a href=\"https://github.com/simonw/openai-to-sqlite/blob/0.3/openai_to_sqlite/cli.py#L248-L303\">the code that makes that happen</a>.</p>\n<h4>Is this a good way of doing things?</h4>\n<p>Even though I managed to get the progress bar working, there are still some big limitations in using a SQL query to do this.</p>\n<p>The first is that any query which triggers external API calls via a custom SQL function is inherently time-consuming. This means running a really long transaction, and since SQLite only accepts a single write at a time this means locking the database for writes for a very long time.</p>\n<p>That’s OK for a command-line script like this when you’re the only user of the database, but it’s not a good idea for databases that are serving other queries—as I frequently do with SQLite and <a href=\"https://datasette.io/\">Datasette</a>.</p>\n<p>A much bigger problem though is what happens when something goes wrong. An <code>update ... set col1 = chatgpt(...)</code> query executes in a single transaction. If you’re running it against 400 rows and something causes an error at row 399, the transaction will rollback and you’ll lose all of the work that DID succeed!</p>\n<p>As such, while this is a fun tool for ad-hoc experiments with OpenAI data enrichment, I don’t think it’s a good long-term solution. A better mechanism would enable each individual API call to be written to storage such that problems in later calls don’t discard data collected earlier on.</p>\n\n\n\n\n</div></div>"

**Response:**
The relevant tools mentioned in the article are:

1. openai-to-sqlite: A command-line tool for enriching data in a SQLite database by running values through an OpenAI model and saving the results. You can find more information about it [here](https://github.com/simonw/openai-to-sqlite/releases/tag/0.3).

2. Datasette: A tool for exploring and publishing data. While not directly related to the conversation search functionality, it is mentioned as being used with openai-to-sqlite. You can find more information about Datasette [here](https://datasette.io/).

3. SQLite: The database used for storing conversations and executing queries.

<details><summary>Metadata</summary>

- Duration: 7358 ms
- Datetime: 2023-08-24T12:59:28.219022
- Model: gpt-3.5-turbo-0613

</details>

**Options:**
```json
{}
```

