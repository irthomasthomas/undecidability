**Prompt:**
**USER_QUESTION:**
Does this support batched inference?
**END_USER_QUESTION**


**Response:**
Yes, llamacpp-python supports batched inference. You can specify the batch size to use per evaluation using the `--n_batch` option. By increasing the batch size, you can process multiple inputs in parallel, which can significantly improve inference performance.

<details><summary>Metadata</summary>

- Duration: 1351 ms
- Datetime: 2024-01-11T20:31:02.197176
- Model: gpt-3.5-turbo-0613

</details>

**Options:**
```json
{}
```

