**Prompt:**
Make this content into a pretty gh markdown note:quote:- [ ] [mit-han-lab/streaming-llm: Efficient Streaming Language Models with Attention Sinks](https://github.com/mit-han-lab/streaming-llm)\n
Abstract\n
#### Suggested labels
#### {   "key": "sparse-computation",   "value": "Optimizing sparse computation for ReLu language models like mixtral moe" } {   "key": "llm-serving-optimisations",   "value": "Strategies to optimize the serving process of large language models" }

**Response:**
> [mit-han-lab/streaming-llm: Efficient Streaming Language Models with Attention Sinks](https://github.com/mit-han-lab/streaming-llm)

**Abstract**

#### Suggested labels
- { "key": "sparse-computation", "value": "Optimizing sparse computation for ReLu language models like mixtral moe" }
- { "key": "llm-serving-optimisations", "value": "Strategies to optimize the serving process of large language models" }

<details><summary>Metadata</summary>

- Duration: 2384 ms
- Datetime: 2024-01-13T10:52:41.764707
- Model: gpt-3.5-turbo-0613

</details>

**Options:**
```json
{"temperature": 0.1}
```

